{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dbc3b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "from torch_geometric.nn import GCNConv, GINConv, GATConv, GATv2Conv\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "from torch_geometric.nn import MessagePassing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "375d58a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'reservoir_data/data_list_vae.pkl'\n",
    "with open(file_path,'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c45c9f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set = train_test_split(data, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9ece625",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=16, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "530edac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, n_features, dim_h, z_dim):\n",
    "        super(GATv2, self).__init__()\n",
    "        self.conv1 = GATv2Conv(n_features, dim_h, edge_dim = 1)\n",
    "        self.conv2 = GATv2Conv(dim_h, dim_h, edge_dim = 1)\n",
    "        self.conv3 = GATv2Conv(dim_h, dim_h, edge_dim = 1)\n",
    "        self.lin = Linear(dim_h, z_dim*2)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        # Node embeddings \n",
    "        h = self.conv1(x, edge_index, edge_attr= edge_attr)\n",
    "        h = h.relu()\n",
    "        h = self.conv2(h, edge_index, edge_attr= edge_attr)\n",
    "        h = h.relu()\n",
    "        h = self.conv3(h, edge_index, edge_attr= edge_attr)\n",
    "\n",
    "        # Graph-level readout\n",
    "        hG = global_mean_pool(h, batch)\n",
    "        h = self.lin(h)\n",
    "        \n",
    "        mu = h[:z_dim]\n",
    "        logvar = h[z_dim:]\n",
    "        \n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c6c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, z_dim, out_dim):\n",
    "        super(GATv2, self).__init__()\n",
    "        self.lin1 = Linear(z_dim, hidden_dim)\n",
    "        self.lin2 = Linear(hidden_dim, hidden_dim)\n",
    "        self.lin3 = Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Node embeddings \n",
    "        h = self.lin1(z)\n",
    "        h = h.relu()\n",
    "        h = self.lin2(h)\n",
    "        h = h.relu()\n",
    "        h = self.lin3(h)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c581d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, Encoder, Decoder, device):\n",
    "        super(Model, self).__init__()\n",
    "        self.Encoder = Encoder\n",
    "        self.Decoder = Decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(self.device)        # sampling epsilon        \n",
    "        z = mean + var * epsilon                          # reparameterization trick\n",
    "        return z\n",
    "                \n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.Encoder(x)\n",
    "        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -> var)\n",
    "        x_hat = self.Decoder(z)\n",
    "        \n",
    "        return x_hat, mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "deeb7688",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        reconstruction, mu, log_var = vae(data[0])\n",
    "        \n",
    "        loss = loss_function(recon, x, mu, logvar)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "def train(model, criterion, epochs, loader, val_loader, save_path):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    update_iter = 500\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs+1):\n",
    "        running_reconstruction_loss = 0\n",
    "        running_KLD_loss = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "\n",
    "        # Train on batches\n",
    "        for data in loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            reconstruction, mu, log_var = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "            \n",
    "            reconstruction_loss, KLD_loss = criterion(reconstruction, data.y, mu, logvar)\n",
    "            \n",
    "            running_reconstruction_loss += reconstruction_loss.item()\n",
    "            running_KLD_loss += KLD_loss.item() \n",
    "            \n",
    "            total_loss += running_reconstruction_loss + running_KLD_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if iteration % update_iter == 0:\n",
    "                # Validation\n",
    "                val_loss = test(model, criterion, val_loader)\n",
    "                # Print metrics every 20 epochs\n",
    "                print(f'Epoch {epoch:>3} | Train Loss: {total_loss/update_iter:} | Reconstruction Loss: {running_reconstruction_loss/update_iter} | KLD loss: {running_KLD_loss/update_iter}' )\n",
    "                losses.append[total_loss/update_iter, running_reconstruction_loss/update_iter, running_KLD_loss/update_iter ]\n",
    "                \n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_model_state = model.state_dict()\n",
    "                    torch.save(best_model_state, save_path) \n",
    "                    \n",
    "                running_reconstruction_loss = 0\n",
    "                running_KLD_loss = 0\n",
    "                total_loss = 0\n",
    "                \n",
    "            iteration += 1\n",
    "            print(iteration, end='\\r')\n",
    "            \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, criterion, loader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        rec,mu,logvar = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        rec_loss, kld_loss = criterion(rec, data.y, mu, logvar) \n",
    "        loss += rec_los.item() + kld_loss.item()\n",
    "\n",
    "    return loss / len(loader)\n",
    "                    \n",
    "            \n",
    "    return model, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ea3ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 125\n",
    "hidden_dim = 32\n",
    "z_dim = 16\n",
    "\n",
    "edge_dim = 1\n",
    "n_nodes = 18\n",
    "\n",
    "\n",
    "encoder = Encoder(n_features,hidden_dim,z_dim)\n",
    "decoder = Decoder(z_dim, n_features*n_nodes + n_nodes**2 + (n_nodes**2)*edge_dim)\n",
    "\n",
    "model = Model(Encoder=encoder,Decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ce249bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 0.00023566046729683876 | Val Loss: 0.00046019235742278397\n",
      "Epoch   0 | Train Loss: 0.0003077992587350309 | Val Loss: 0.0005208745133131742\n",
      "Epoch   0 | Train Loss: 0.0003799987316597253 | Val Loss: 0.0004448906984180212\n",
      "Epoch   0 | Train Loss: 0.0004534148029051721 | Val Loss: 0.00045804408728145063\n",
      "Epoch   0 | Train Loss: 0.0005272722919471562 | Val Loss: 0.0004370369715616107\n",
      "Epoch   0 | Train Loss: 0.000599189312197268 | Val Loss: 0.00045981016592122614\n",
      "Epoch   1 | Train Loss: 1.632763451198116e-05 | Val Loss: 0.0004718992277048528\n",
      "Epoch   1 | Train Loss: 8.670402894495055e-05 | Val Loss: 0.0004062704392708838\n",
      "Epoch   1 | Train Loss: 0.00014990601630415767 | Val Loss: 0.00044218855327926576\n",
      "Epoch   1 | Train Loss: 0.00021457116235978901 | Val Loss: 0.00042080131242983043\n",
      "Epoch   1 | Train Loss: 0.00028681097319349647 | Val Loss: 0.0004002690257038921\n",
      "Epoch   1 | Train Loss: 0.0003533469862304628 | Val Loss: 0.0003725403221324086\n",
      "Epoch   1 | Train Loss: 0.0004174591740593314 | Val Loss: 0.00044359604362398386\n",
      "Epoch   2 | Train Loss: 3.157119863317348e-05 | Val Loss: 0.0003761896805372089\n",
      "Epoch   2 | Train Loss: 9.690661681815982e-05 | Val Loss: 0.0003717150539159775\n",
      "Epoch   2 | Train Loss: 0.00015572001575492322 | Val Loss: 0.00038088340079411864\n",
      "Epoch   2 | Train Loss: 0.00021442020079120994 | Val Loss: 0.0003941834147553891\n",
      "Epoch   2 | Train Loss: 0.0002904314605984837 | Val Loss: 0.0004776665009558201\n",
      "Epoch   2 | Train Loss: 0.00036577408900484443 | Val Loss: 0.0005255515570752323\n",
      "Epoch   2 | Train Loss: 0.00044182693818584085 | Val Loss: 0.0004469025006983429\n",
      "Epoch   3 | Train Loss: 5.48303869436495e-05 | Val Loss: 0.000560011132620275\n",
      "Epoch   3 | Train Loss: 0.00012935645645484328 | Val Loss: 0.0004611232434399426\n",
      "Epoch   3 | Train Loss: 0.00020724732894450426 | Val Loss: 0.00047623703721910715\n",
      "Epoch   3 | Train Loss: 0.0002782160008791834 | Val Loss: 0.00047630202607251704\n",
      "Epoch   3 | Train Loss: 0.00035882240626960993 | Val Loss: 0.0005131693324074149\n",
      "Epoch   3 | Train Loss: 0.00043361319694668055 | Val Loss: 0.0004809983365703374\n",
      "Epoch   3 | Train Loss: 0.0005127150798216462 | Val Loss: 0.000520185858476907\n",
      "Epoch   4 | Train Loss: 7.778344297548756e-05 | Val Loss: 0.0004827160737477243\n",
      "Epoch   4 | Train Loss: 0.0001519813813501969 | Val Loss: 0.0004850772093050182\n",
      "Epoch   4 | Train Loss: 0.00022575235925614834 | Val Loss: 0.0005024363636039197\n",
      "Epoch   4 | Train Loss: 0.0003034984110854566 | Val Loss: 0.0005762548535130918\n",
      "Epoch   4 | Train Loss: 0.0003816032549366355 | Val Loss: 0.0004774113476742059\n",
      "Epoch   4 | Train Loss: 0.00045816777856089175 | Val Loss: 0.0004892033175565302\n",
      "Epoch   5 | Train Loss: 1.867314313130919e-05 | Val Loss: 0.0005096073145978153\n",
      "Epoch   5 | Train Loss: 9.822354331845418e-05 | Val Loss: 0.0004856899904552847\n",
      "Epoch   5 | Train Loss: 0.00017022200336214155 | Val Loss: 0.0004767327627632767\n",
      "Epoch   5 | Train Loss: 0.00024435410159640014 | Val Loss: 0.0004753930261358619\n",
      "Epoch   5 | Train Loss: 0.000324051856296137 | Val Loss: 0.0005089900805614889\n",
      "Epoch   5 | Train Loss: 0.00040116417221724987 | Val Loss: 0.0004859608307015151\n",
      "Epoch   5 | Train Loss: 0.0004773755499627441 | Val Loss: 0.00047884404193609953\n",
      "Epoch   6 | Train Loss: 3.885340629494749e-05 | Val Loss: 0.0005404297262430191\n",
      "Epoch   6 | Train Loss: 0.00011703651398420334 | Val Loss: 0.000532883161213249\n",
      "Epoch   6 | Train Loss: 0.00019098205666523427 | Val Loss: 0.0004974267794750631\n",
      "Epoch   6 | Train Loss: 0.00026368233375251293 | Val Loss: 0.00047665979946032166\n",
      "Epoch   6 | Train Loss: 0.0003456464328337461 | Val Loss: 0.0004756868293043226\n",
      "Epoch   6 | Train Loss: 0.0004218715475872159 | Val Loss: 0.000514841522090137\n",
      "Epoch   6 | Train Loss: 0.0005009571323171258 | Val Loss: 0.0006852760561741889\n",
      "Epoch   7 | Train Loss: 5.834039984620176e-05 | Val Loss: 0.00047482686932198703\n",
      "Epoch   7 | Train Loss: 0.0001353229017695412 | Val Loss: 0.0004750803636852652\n",
      "Epoch   7 | Train Loss: 0.0002091461356030777 | Val Loss: 0.0004748534120153636\n",
      "Epoch   7 | Train Loss: 0.00028084407676942647 | Val Loss: 0.0005640716990455985\n",
      "Epoch   7 | Train Loss: 0.00036262316280044615 | Val Loss: 0.00048009041347540915\n",
      "Epoch   7 | Train Loss: 0.0004378454468678683 | Val Loss: 0.00047629771870560944\n",
      "Epoch   7 | Train Loss: 0.0005164984031580389 | Val Loss: 0.00047536782221868634\n",
      "Epoch   8 | Train Loss: 7.849742542020977e-05 | Val Loss: 0.00047480282955802977\n",
      "Epoch   8 | Train Loss: 0.00015355524374172091 | Val Loss: 0.00047645074664615095\n",
      "Epoch   8 | Train Loss: 0.00022748859191779047 | Val Loss: 0.00047992670442909\n",
      "Epoch   8 | Train Loss: 0.0003057195572182536 | Val Loss: 0.00048647605581209064\n",
      "Epoch   8 | Train Loss: 0.000384725775802508 | Val Loss: 0.00048280495684593916\n",
      "Epoch   8 | Train Loss: 0.0004612559569068253 | Val Loss: 0.0004749012878164649\n",
      "Epoch   9 | Train Loss: 1.8704295143834315e-05 | Val Loss: 0.0005609237123280764\n",
      "Epoch   9 | Train Loss: 9.849778143689036e-05 | Val Loss: 0.0005056486697867513\n",
      "Epoch   9 | Train Loss: 0.0001708267955109477 | Val Loss: 0.0005611481610685587\n",
      "Epoch   9 | Train Loss: 0.00024516956182196736 | Val Loss: 0.0004990833112969995\n",
      "Epoch   9 | Train Loss: 0.0003249564324505627 | Val Loss: 0.0005133496015332639\n",
      "Epoch   9 | Train Loss: 0.0004016979946754873 | Val Loss: 0.0005027439910918474\n",
      "Epoch   9 | Train Loss: 0.0004782815813086927 | Val Loss: 0.00047740343143232167\n",
      "Epoch  10 | Train Loss: 3.894343535648659e-05 | Val Loss: 0.0005600285949185491\n",
      "Epoch  10 | Train Loss: 0.00011728794925147668 | Val Loss: 0.0005449271411634982\n",
      "Epoch  10 | Train Loss: 0.00019061074999626726 | Val Loss: 0.0004965451662428677\n",
      "Epoch  10 | Train Loss: 0.00026337362942285836 | Val Loss: 0.00047541409730911255\n",
      "Epoch  10 | Train Loss: 0.00034634527401067317 | Val Loss: 0.00047560635721310973\n",
      "Epoch  10 | Train Loss: 0.00042250179103575647 | Val Loss: 0.0005241221515461802\n",
      "Epoch  10 | Train Loss: 0.0005015985225327313 | Val Loss: 0.0006626645918004215\n",
      "37125\r"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "epochs = 10\n",
    "\n",
    "save_path = 'reservoir_data/res16_best_model.pth'\n",
    "\n",
    "gcn = gcn.to(device)\n",
    "\n",
    "gcn, train_losses, val_losses = train(gcn, epochs, train_loader, test_loader, save_path)\n",
    "\n",
    "\n",
    "#test_loss = test(gcn, test_loader)\n",
    "#print(f'Test Loss: {test_loss:.2f} | Test Acc: {test_acc*100:.2f}%')\n",
    "#print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
